1. 腾讯timi，以人类为中心的RL。智能体不能将游戏的回报作为学习的目标，而是应该更倾向于照顾人类的游戏体验。本文提出了一种以人类为中心的智能体回报的建模体系。首先通过SL用人类数据训练出人类policy。自我为中心的agent和人类的游戏数据作为baseline。agent策略的目的是最大化当前策略和人类合作得到的return对比baseline的优势。共两个网络，一个策略网络，一个估计return的value网络。
1. ==facebook 能够玩《外交》游戏的agent==。这是一个需要玩家发言相互磋商谈判的游戏。首先收集了大量的游戏数据，对已有的预训练模型做微调，使得智能体能够说出游戏中的对话。在数据集中标注好每个对话所对应的意图，便于智能体将意图和自然语言做对应。模型有两个部分组成：策略规划模块和对话模块。对话模块是pretrained语言模型+使用人类玩游戏历史微调的语言模型；输入对话历史和strategy输出的intent，输出新的语言。strategy模块的核心是value网络和policy网络组成的self-play RL智能体。输入标注好意图的对话历史和state，利用rl做决策，输出当前agent的intent和具体action。意图intent：发送者和接受者最有可能采取的一组行动。意图和message发送模块相结合，发出内容可控的且具有泛化性的信息。
1. 清华 2024 训练==一个ToM模型==，在human-ai合作任务中预测ai agent的下一步动作（意图），增强agent的可解释性。本文未涉及到agent的主动通信。训练过程：SL。首先通过数据集将action和state的embedding结合起来，对其使用transformer输出当前状态下下一个动作的预测。属于外部模型帮助human预测与之合作的智能体的意图。该文章related work有引文

> 我打算将人机合作方向的论文分为两个方面，意图推断和通信。区别在于双方主动进行通信；智能体通过某种手段获取其他人的意图
>
> 关于意图推断方面，大多数文章聚焦于机器人领域，即机器人在接收到意图之后如何将其转化成运动学方程（5）有些工作是如何将机器人任务分解为动作，并将动作标签展示出来（4）。

4. ==UCLA 2019 science robostic== 构建了触觉模型和符号动作模型两种模型，使得机器人模仿人类生成动作。并基于生成的动作序列生成每一步的解释。触觉模型能够使得机器人仿照人类的触觉，如压力和扭力，输出动作；而符号动作模型能够预测动作序列的下一个动作，符号动作模型首先将人类的动作序列视作句子，根据句子生成语法规则，再使用语法规则和已有的动作序列预测下一个动作是什么。动作生成的同时也对当前采取的动作进行解释和动画演示。**这属于什么**，动作生成的时候就带有可解释性，具体解释的时候直接输出动作就好。便于人类理解机器人的意图。
4. 本文提出了一种机器人和操作者双向通信的架构。人类下发指令，指令包括任务目标和约束等。机器人接受任务后首先在模拟环境中演练一下，如果可行则执行，并发送反馈信息给人类；如果不可行，则发送信息给人类希望不执行或调整约束。**双向通信信道是商业化的语音助手，此处无创新**。
4. 使用增强现实技术使得机器人交流自己的动作意图，纯实验性质 2017年
4. 2022年 科罗拉多大学 构造了一个人机合作的环境，无人机具有全局视野，辅助人类进行挖矿。双方沟通的内容是全局环境的概率质量函数（PMF），即全局状态的价值如何。人机双方都是MDP问题。本文通过设计双方的奖励函数，通过不断交流PMF增强人类对环境的理解。通信方式是：AR。无人机将全局环境的PMF转化成AR发送给人类，人类会发现某些地块更亮，这表示某些地块的价值越高；或者无人机干脆将价值高的地块用导引线连接在一块。
4. ==science robotics== UCLA 2022 人机合作任务中使用交流进行人机价值对齐——XAI+intent推测。agent为了有效协助人类，需要作为“听者”和“说者”。首先，智能体需要从人类的反馈中提取意图，推断人类的价值观，并由此调整他们的策略；其次，agent需要根据价值推断，有效的解释他们已经做了什么以及计划做什么
4. 2012年 纯讲实验的，不重要、
4. 2020年 布朗大学 建立了一个测试环境，测试人类的手势、眼神等表达和自然语言相比哪个比较适合让机器人理解人类的意图。实验结果证明后者更合适（未完）
4. 2022年 设计一个实验，使得MR技术可以显示该机器人的未来的动作，帮助人类预期得到机器人的下一步意图。**纯工程无创新**。
4. 该文章**通过实验**表明XAI可以在规模可变队伍中进行通信时做出增益。全文讲实验无公式。
4. 没什么用
4. 没什么用
4. 纯讲实验的，没什么用
4. 介绍的通信机制，没有涉及通信原理

---

本模块总结：

本模块聚焦于意图捕获，尤其是人类和机器人系统的意图捕获的ui设计。

本模块有价值的工作：2348

第二篇，agent在测试环境中与其他智能体博弈，通过大语言模型微调来学会具体的沟通和理解，使用对话历史生成策略，并且从历史对话中捕捉智能体在当前状态下的意图，使用意图微调大语言模型的对话输出。本文的对话模块：微调的领域大语言模型+捕获好的意图；策略模块：RL训练当前episode的对话历史。

第三篇，创建agent，辅助人类对对手的动作进行预测。预测来源是对手智能体的历史，输入state&action的联合embedding，输出智能体动作的预测。中间模型是transformer

第四篇，机器人对自己的动作生成解释。机器人首先从人类的动作中模仿学习+encoder&decoder，学习得到动作标签，进而学会自己产生动作以及自己的动作属于什么标签。我认为可以泛化一下，任何策略生成时，如果是模仿学习的，都可以学习的过程中引入标签，再在执行的时候传递出来作为人机交互的通信

第八篇，

总的来说，本模块对于通信的文章中，主要聚焦于ai的可解释性，和意图捕获（双向）。这是符合直觉的。双向意图捕获能够使得机器人和人类相互交换对方不知道的信息，即视为通信；可解释性也助于基于算法的智能体与其他智能体合作。

---

> 2024.10.3更新

根据讨论，第二篇过于工程，不建议采纳；第三篇较简单，但是方法论和实验环境可以参考；第四篇比较好，可以细读，仔细研究其中智能体怎样根据观察学习到人类的动作，并将自己的动作过程表述出来；第八篇也具有“将自己的意图表述出来”的模块，可以具体研究下这个模块如何实现，但是关于第八篇交互的部分，可能规模太大，可以先不关注。
