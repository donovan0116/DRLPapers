## MCC-related

1. 腾讯timi，以人类为中心的RL。智能体不能将游戏的回报作为学习的目标，而是应该更倾向于照顾人类的游戏体验。本文提出了一种以人类为中心的智能体回报的建模体系。首先通过SL用人类数据训练出人类policy。自我为中心的agent和人类的游戏数据作为baseline。agent策略的目的是最大化当前策略和人类合作得到的return对比baseline的优势。共两个网络，一个策略网络，一个估计return的value网络。
1. ==facebook 能够玩《外交》游戏的agent==。这是一个需要玩家发言相互磋商谈判的游戏。首先收集了大量的游戏数据，对已有的预训练模型做微调，使得智能体能够说出游戏中的对话。在数据集中标注好每个对话所对应的意图，便于智能体将意图和自然语言做对应。模型有两个部分组成：策略规划模块和对话模块。对话模块是pretrained语言模型+使用人类玩游戏历史微调的语言模型；输入对话历史和strategy输出的intent，输出新的语言。strategy模块的核心是value网络和policy网络组成的self-play RL智能体。输入标注好意图的对话历史和state，利用rl做决策，输出当前agent的intent和具体action。意图intent：发送者和接受者最有可能采取的一组行动。意图和message发送模块相结合，发出内容可控的且具有泛化性的信息。
1. 清华 2024 训练==一个ToM模型==，在human-ai合作任务中预测ai agent的下一步动作（意图），增强agent的可解释性。本文未涉及到agent的主动通信。训练过程：SL。首先通过数据集将action和state的embedding结合起来，对其使用transformer输出当前状态下下一个动作的预测。属于外部模型帮助human预测与之合作的智能体的意图。该文章related work有引文

> 我打算将人机合作方向的论文分为两个方面，意图推断和通信。区别在于双方主动进行通信；智能体通过某种手段获取其他人的意图
>
> 关于意图推断方面，大多数文章聚焦于机器人领域，即机器人在接收到意图之后如何将其转化成运动学方程（5）有些工作是如何将机器人任务分解为动作，并将动作标签展示出来（4）。

4. ==UCLA 2019 science robostic== 构建了触觉模型和符号动作模型两种模型，使得机器人模仿人类生成动作。并基于生成的动作序列生成每一步的解释。触觉模型能够使得机器人仿照人类的触觉，如压力和扭力，输出动作；而符号动作模型能够预测动作序列的下一个动作，符号动作模型首先将人类的动作序列视作句子，根据句子生成语法规则，再使用语法规则和已有的动作序列预测下一个动作是什么。动作生成的同时也对当前采取的动作进行解释和动画演示。**这属于什么**，动作生成的时候就带有可解释性，具体解释的时候直接输出动作就好。便于人类理解机器人的意图。
4. 本文提出了一种机器人和操作者双向通信的架构。人类下发指令，指令包括任务目标和约束等。机器人接受任务后首先在模拟环境中演练一下，如果可行则执行，并发送反馈信息给人类；如果不可行，则发送信息给人类希望不执行或调整约束。**双向通信信道是商业化的语音助手，此处无创新**。
4. 使用增强现实技术使得机器人交流自己的动作意图，纯实验性质 2017年
4. 2022年 科罗拉多大学 构造了一个人机合作的环境，无人机具有全局视野，辅助人类进行挖矿。双方沟通的内容是全局环境的概率质量函数（PMF），即全局状态的价值如何。人机双方都是MDP问题。本文通过设计双方的奖励函数，通过不断交流PMF增强人类对环境的理解。通信方式是：AR。无人机将全局环境的PMF转化成AR发送给人类，人类会发现某些地块更亮，这表示某些地块的价值越高；或者无人机干脆将价值高的地块用导引线连接在一块。
4. ==science robotics== UCLA 2022 人机合作任务中使用交流进行人机价值对齐——XAI+intent推测。agent为了有效协助人类，需要作为“听者”和“说者”。首先，智能体需要从人类的反馈中提取意图，推断人类的价值观，并由此调整他们的策略；其次，agent需要根据价值推断，有效的解释他们已经做了什么以及计划做什么
4. 2012年 纯讲实验的，不重要、
4. 2020年 布朗大学 建立了一个测试环境，测试人类的手势、眼神等表达和自然语言相比哪个比较适合让机器人理解人类的意图。实验结果证明后者更合适（未完）
4. 2022年 设计一个实验，使得MR技术可以显示该机器人的未来的动作，帮助人类预期得到机器人的下一步意图。**纯工程无创新**。
4. 该文章**通过实验**表明XAI可以在规模可变队伍中进行通信时做出增益。全文讲实验无公式。
4. 没什么用
4. 没什么用
4. 纯讲实验的，没什么用
4. 介绍的通信机制，没有涉及通信原理

---

本模块总结：

本模块聚焦于意图捕获，尤其是人类和机器人系统的意图捕获的ui设计。

本模块有价值的工作：2348

第二篇，agent在测试环境中与其他智能体博弈，通过大语言模型微调来学会具体的沟通和理解，使用对话历史生成策略，并且从历史对话中捕捉智能体在当前状态下的意图，使用意图微调大语言模型的对话输出。本文的对话模块：微调的领域大语言模型+捕获好的意图；策略模块：RL训练当前episode的对话历史。

第三篇，创建agent，辅助人类对对手的动作进行预测。预测来源是对手智能体的历史，输入state&action的联合embedding，输出智能体动作的预测。中间模型是transformer

第四篇，机器人对自己的动作生成解释。机器人首先从人类的动作中模仿学习+encoder&decoder，学习得到动作标签，进而学会自己产生动作以及自己的动作属于什么标签。我认为可以泛化一下，任何策略生成时，如果是模仿学习的，都可以学习的过程中引入标签，再在执行的时候传递出来作为人机交互的通信

第八篇，

总的来说，本模块对于通信的文章中，主要聚焦于ai的可解释性，和意图捕获（双向）。这是符合直觉的。双向意图捕获能够使得机器人和人类相互交换对方不知道的信息，即视为通信；可解释性也助于基于算法的智能体与其他智能体合作。

---

> 2024.10.3更新

根据讨论，第二篇过于工程，不建议采纳；第三篇较简单，但是方法论和实验环境可以参考；第四篇比较好，可以细读，仔细研究其中智能体怎样根据观察学习到人类的动作，并将自己的动作过程表述出来；第八篇也具有“将自己的意图表述出来”的模块，可以具体研究下这个模块如何实现，但是关于第八篇交互的部分，可能规模太大，可以先不关注。

## TN-related

1. 本文开发了一个方法，将智能体游戏环境中的state和action转换成自然语言。环境选取的是青蛙过河。翻译方法：首先，人类玩这个游戏，在玩的过程中采集自然语言的数据集；然后，使用数据集语料库+encoder-decoder根据state和action生成解释。弱点：方法过于简单，且文章中没有公式

2. 2022年 CMU ==在对话系统中==，智能体通过人类的语言推断出背后隐藏的奖励模型。使得智能体具备一种能力：不仅根据人类的语言完成人类的工作，还能根据之前的上下文信息优化工作的完成。比如说订票系统，不仅能根据语言筛选出合适的机票，还能从符合条件的机票中选择人类最偏好的。模型可以从自然语言中推断出奖励函数，在重复的交互中不断提高奖励估计，并使用推断的奖励在僵持环境中准确选择最佳行动。简单来说，是通过推断人类语言中的偏好，实现IRL的效果。**工作内容不错，但agent揣测人类intent可能不是现阶段的重点**。、

3. 2023 ==CMU==目前为止第一个对agent生成的通信模型进行可解释性转换的算法，效果是对通信model进行可解释化，这是本文的第一个贡献；本文的第二个贡献：验证了通信的稀疏性有助于提高通信效果。本文首先提出了一个类似于IC3Net的agent通信模型，然后使用这个通信模型进行了两个环境的测试。这个通信模型将lstm的隐变量压缩编码成向量，作为通信信息结合门控单元发送给其他智能体。在本文第四章，作者将这些向量化的通信信息映射到二维空间上，发现这些信息确实编码了部分可观察环境中的位置信息，上述作为通信可解释性的一部分。本文第五章描述了人类如何与刚形成的智能体通信。人类通过学习agent训练生成的通信符号，并使用这些符号进而与智能体进行协作。

4. 2020年 google 提出了一种训练框架，能够在不预先定义reward model和模仿学习的情况下，智能体通过和人类进行**非结构化通信的方式**从人类处进行训练（从人类处获得需要完成任务的任务信息），训练好的agent可以作为人类的助手帮助人类完成一些工作。从具体实现上来讲，在训练的过程中，主智能体和从智能体放在同一个网络中训练策略，以此作为联合训练；对于主智能体和从智能体之间的部分可观察的范围限制有所不同，以此限制主智能体和从智能体的区别。总结来看，这篇文章的内容不算很重要，同样也是智能体通过协作和交互来猜其他智能体的奖励函数，交互的方式是放在一起训练（DRQN）

5. 同1

6. ==系统训练了一个问答环境下使用自然语言进行猜图片的智能体架构==。智能体分为两部分：q-bot和a-bot。两个bot都编码历史对话信息，qbot根据历史信息提问；abot根据历史信息、问题和图片进行回答，q来猜a的图片是哪个。双方对话使用自然语言。**训练过程**：首先，双方使用数据集通过SL进行隔离训练，除了语句的MSE之外，q-bot还通过猜出图片的MSE训练；其次，双方使用强化学习self-play训练；最后，由于上述方法训练出的智能体产生的自然语言不够能够使人类理解，因此引入“社区化”训练，即qa过程引入一对多和多对一的RL训练。**本文贡献**：创建了一种自然语言交流的方式。

7. 综述，总结了至今（2023年）对于human-robot teaming现存的工作和挑战，其中包括communication的挑战

8. 提出一个框架，对深度神经网络分类器中的某些神经元的作用进行判别，能够识别出某个神经元能够分类图片的哪个部分。**属于图像分类器的可解释性**。

9. 斯坦福 nips 2020 对分类模型中的神经元生成组合式解释的方法。对于已经完成训练的分类神经模型，对于每个神经元，首先找到使其激活的特征，然后使用启发式搜索方法找到符合其特征的原子标签，然后随着启发式束搜索，逐渐扩展原子标签，将原子标签相互之间使用逻辑连接词连接在一起，构成对神经元对应特征的类似自然语言解释。文章提出的方法是对分类模型的神经元进行事后的解释生成，有助于对推理和决策的可解释性。

10. 本文提出了一种基于互信息的evaluation函数，这个函数通过计算用户的指令信号与状态转移之间的互信息来衡量用户指令信号对状态转移的影响，进而评估这个interface的效果具体如何。对interface进行评估有助于量化用户指令的杂讯，进而提升interface的水平并协作效率

11. 关于模型可解释性的工作。本模型提出了一种模块化的方法，试图解释模型的推理过程。Stack Neural Module Networks (Stack-NMN) 的训练流程设计目的是在不依赖专家指导的情况下，通过模块化的方式进行推理任务的训练。以下是该模型的训练流程的关键步骤：

    ### 1. **输入数据的编码**

    - **问题编码**：首先，输入的问题通过双向LSTM（BiLSTM）编码为一个维度为d的隐藏状态序列 [h1,h2,…,hS][h_1, h_2, \dots, h_S][h1,h2,…,hS]，其中 SSS 是问题中的词的数量。每个词的隐藏状态是前向和后向LSTM的输出的连接结果。
    - **图像编码**：图像被输入到一个预训练的卷积神经网络（CNN）中（如ResNet-101），该CNN负责提取图像的特征图（feature map），作为视觉输入。

    ### 2. **模块布局的预测**

    - **布局控制器（Layout Controller）**：模型通过一个布局控制器，将问题分解为多个子任务。具体地，布局控制器会预测每个时间步（t）下需要执行的模块的权重 w(t)\mathbf{w}(t)w(t)，并为每个模块生成文本参数 ct\mathbf{c}_tct。这些模块包括如“查找”、“过滤”、“比较”等功能，用来解决特定的子任务。
    - **软布局选择**：Stack-NMN采用“软布局”，这意味着在每个时间步，所有模块都被执行，但每个模块的输出会根据其预测的权重 w(t)\mathbf{w}(t)w(t) 进行加权平均。这使得整个推理过程是可微的，能够通过反向传播进行训练，而不依赖强化学习方法。

    ### 3. **模块的执行与推理**

    - **模块执行**：每个模块根据输入的图像特征图和文本参数执行其子任务。模块的输出可以是新的图像注意力区域或其他中间推理结果。每个时间步的模块执行结果会通过一个可微的栈（LIFO栈）进行存储与传递，以便在后续的推理步骤中引用。
    - **栈操作**：模块可以从栈中“弹出”之前的推理结果，并将新的推理结果“压入”栈中。栈的结构确保了推理过程的层次化，使得模块之间可以相互依赖和传递信息。

    ### 4. **损失函数与训练目标**

    - **任务损失**：在VQA任务中，模型的最终目标是预测问题的正确答案。模型在推理过程中生成的答案得分通过softmax交叉熵损失与真实答案进行比较。在REF（指代表达定位）任务中，模型的目标是生成一个正确的边界框，并通过边界框回归损失和图像注意力损失来优化定位精度。
    - **无专家布局监督的训练**：Stack-NMN与传统的模块化网络不同，能够在没有专家指导的情况下训练布局策略。模型的所有参数（包括布局控制器的参数、模块的参数和栈的参数）通过任务的监督信号进行端到端的反向传播优化。

    ### 5. **联合训练（多任务学习）**

    - **VQA和REF的联合训练**：Stack-NMN可以通过共享模块来同时处理多个任务。通过多任务学习，模型能够从不同任务中学习到通用的推理步骤，例如“查找物体”或“处理关系”。这些通用步骤能够在不同任务之间共享，从而提高模型在多个任务上的表现。

    ### 6. **测试与推理**

    - **软布局到离散布局的转换**：在测试阶段，模型的软布局选择可以通过最大化选择（argmax）离散化为单一的模块布局。这种离散化的选择与训练时的软布局一致，模型依然能够保持良好的推理性能。

    ### 总结

    该创新方法的训练流程通过软布局的模块化推理方式实现了端到端的可微训练，不需要依赖专家的布局监督。这种设计不仅提升了推理的透明度和可解释性，还保持了较高的任务精度，能够有效地处理复杂推理任务如视觉问答和指代表达定位。

12. 设计了两个实验，用来验证人类-robot队伍下，人类如何感知机器人之间的通信。实验证明，在人类在场的情况下，机器人之间的交流最好通过自然语言（纯实验）

13. 三页的综述

14. 本文描述了团队沟通协作的一些实验。实验表明，高效的智能体团队在沟通时需要谨慎的交换信息；信息内容和发送时机需要结合对其他智能体内部状态的揣测。高效的智能体团队交流的信息往往是与目标（或自己的意图）相关的信息，而不是与世界状态相关的信息

## TL-related

1. 一个基于神经网络的翻译系统

2. ==机器人的分层指令执行系统==

   **接收高层次指令**：机器人首先接收自然语言描述的高层次指令。这些指令通常是抽象的，例如“把两个光盘放进卧室的保险箱”，并且不包含具体的行动步骤。

   **高层控制器（High-level Controller）生成子目标**：高层控制器（$\pi_H$）根据输入的高层次指令生成一系列子目标。每个子目标代表执行高层次任务所需的一步，例如“拿起光盘”、“打开保险箱”等。这些子目标可以被看作是整个任务分解成更小的、可实现的部分。

   **低层控制器（Low-level Controller）执行子目标**：低层控制器（$\pi_L$）根据当前子目标执行具体的操作。这包括移动到目标位置、操控物体（如打开、拾取）等。低层控制器通过近距离推理，逐步完成每个子目标。

   **观察模型更新空间语义表示**：执行动作之后，机器人从环境中接收新的观测数据（例如 RGB 图像），并通过观察模型更新其空间语义表示。这一表示存储了环境中各对象的位置和状态信息，以及机器人自身的位置等。这使得机器人在需要时可以对过去观察到的对象进行推理，即使这些对象已经不在视野中。

   **高层和低层控制器的动态调整**：基于更新后的空间语义表示，高层控制器可以评估当前任务的进展情况，并决定接下来要执行的子目标（例如，如果某个子目标未成功，则可能重新尝试或选择其他子目标）。同样，低层控制器利用更新的环境信息继续完成指定的子目标，直到其成功或失败。

3. ==综述==，描述了NLP与强化学习的结合，将相关的工作分为两类：自然语言条件下的RL和自然语言辅助RL。本文值得回头重看

4. 机器翻译的工作

5. 训练人工神经网络驾驶汽车

6. AAAI 2018 一个端到端的、可训练的、基于门控注意力机制的指令理解系统。模型的输入是游戏环境（pixel）和自然语言指令，输出策略分布$\pi$. 游戏环境和自然语言通过gate-attention模块进行融合，输出状态标识，然后通过A3C的策略学习模块进行策略学习。

7. iclr 2021 一种==language-goal-behavior==（LGB）的强化学习架构。智能体首先学习G-B过程，从环境中获取goal，然后理解goal并转换成行动；第二阶段，从language中学习goal；最后在执行时，将自然语言instruction中的goal和环境中获取的goal叉乘输入智能体算法来获得行为

8. 2020年 deepmind ==类似于6==，融合language和vision的特征输入RL模型，输出策略；输入language的使用大语言模型，使得模型能理解zero-shot的语言

9. 2021年 ==MIT== 提出一种使用离散信息作为通信协议的智能体之间的通信方法。人类的自然语言其实是离散形式的，因此多智能体系统使用这种离散形式的协议，可以训练完成后使用聚类算法进行翻译，然后使得人类可以加入协作。具体实现方式：将[observation:对于消息注意力:连续消息:离散消息]作为一个输入，输入PPO等算法进行强化学习训练。最后使用聚类算法，使得人类能够将智能体的离散消息映射到实际意义上，当人类参与协作时可以使用这些离散语言。

10. 双向RNN实现的自编码器，用于神经机器翻译

11. 翻译系统综述，值得参考

12. 2019年 ==康奈尔== 一个视觉+指令指导的智能体强化学习model。模型分为两部分：第一部分，observation融合NL指令生成对于目标位置的特征；第二部分，模型输入目标位置的特征，经过RNN处理，输出对应的动作。前者使用SL，后者RL。本文测试环境值得记录一下，属于人机协作中人指导agent执行动作，agent理解。

13. 阅读过，智能体之间通信的工作，双方玩猜图片的游戏，双方使用图片的特征进行交流

14. 葡萄牙波尔图大学IJCNN A3C2模型，多加了一个通信网络的、多智能体的A3C，智能体之间的通信

15. 神经机器翻译的综述，包含对embedding算法的总结

16. nips2019 UCL 文中提出两种目标函数，用在智能体之间MARL通信。两个目标函数分别作用于speaker和listener。前者直观上提高speaker的说话积极性；后者让listener听到message之后积极采取不一样的动作。

17. 没太看懂，但感觉不涉及人机交互问题
