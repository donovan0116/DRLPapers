1. 2024年9月 匹兹堡大学+本田 智能体在类IC3Net网络中学会使用自然语言通信。方法：在训练智能体通信时，同步使用LLM Agent。LLM输入环境的obs，输出可能的action和自然语言的communication c，形成（o，a，c）的数据集。当智能体学会IC3Net的通信后，智能体的RL loss在原始的基础上增加agent的message和数据集message的差距一项，使得智能体发出的通信接近人类语言，进而使用自然语言通信。

2. 2022年4月 UCB 在订机票任务中，智能体不仅从人类的语言中获得动作指令，还尝试在长期上推测人类的偏好参数，并使用这些偏好参数指导智能体动作的生成。**建模：人类说出一句话的概率=人类基于想要完成的动作说出这句话的概率+人类基于隐含价值判断想要说出这句话的概率**。方法：贝叶斯推断。智能体根据动作和偏好推断出偏好向量，与航班的特征向量点积作为该选择的价值，根据价值选择价值最高的航班选择。文中还建立了一个自然语言-偏好的数据集，用来模型的验证。

3. 2020.5 deepmind 在对话猜图任务中，智能体语言相关的学习主要分为两部分：说出能够符合语法的自然语言；说出能够满足任务需要的语言。本文对于两个方面分别建模了loss进行训练，使用预训练的大模型+自然语言数据集来训练智能体说出符合语义的话语；使用强化学习对奖励函数学习满足任务要求的自然语言。然而，后者会更改奖励，进而导致输出的自然语言漂移。为了解决这样的问题：

   1. 首先分别训练，然后利用强化学习前后的KL散度限制漂移程度。
   2. 使用联合加权loss共同训练
   3. 首先训练微调的智能体，使得智能体能说人话，然后启发式生成若干个候选句子，再通过设计奖励信号对候选句子排名，选择最符合任务的句子。（重排序可以看做强化学习选择动作来选择句子，使用reinforce训练的）

   实践表明第三个最好。

4. 2024.9 openai，使用逆强化学习根据数据库微调大语言模型。将语料库视为专家轨迹，将一个词看做动作，句子中之前的词视作之前的动作，使用逆强化学习结合原有的MLE优化下一步词汇的生成。如此解释的目的是，在词汇生成的时候同时关注之前的词汇和未来可能的价值。选择的逆强化学习方法是：逆软Q学习。具体实现方法：将MLE的优化目标函数的基础上添加 value 的 TD error。
   $$
   \min_\pi J(\pi) = -\min_{v_r, \pi_r} \mathbb{E}_{\mu_E} \left[ f^*(-r(v_r, \pi_r)) + r(v_r, \pi_r) - \lambda \log \pi_r \right]
   $$
   相比之下，原始的MLE：
   $$
   \min_\pi J_{\text{MLE}}(\pi) = - \sum_{x \in D} \sum_{i=0}^{N} \log \pi(x_i | x_0, \dots, x_{i-1})
   $$
5. 2020.4 UCLA VCLA实验室，目的是使得智能体能够通过观察环境推测出人类产生的误解。（例如：环境中有一个杯子，然后人类离开了，另一个人过来换了一个一模一样的杯子，最开始的人会认为这是同一个杯子。）使用的方法是解析图结构。首先智能体使用解析图，将环境中的人和物体信息都记录下来；由于环境是不完全的，智能体和其他智能体联合将各自的解析图融合在一起形成joint解析图。这个联合解析图记录了整个场景的全部的真实信息。同时，智能体会记录人类的信念信息，即，所有物品的状态只有被人类观察到时，才会发生变化，人类看不到的变化只会被真实信息解析图更新，而不会被人类解析图更新。最后，对比人类解析图和真实信息解析图就能得到人类的错误信念信息。感觉**使用解析图记录全部信息并合并解析图**这种记录信息的数据结构值得参考；**推测人类误解，以及根据人类误解推测人类奖励的变化也值得参考**。
6. 2023.9 UCLA 开发了一个用来验证LLMs写作能力的游戏环境基础设施，属于数据及开发类型。
7. 
