1. 一个基于神经网络的翻译系统

2. 机器人的分层指令执行系统

   **接收高层次指令**：机器人首先接收自然语言描述的高层次指令。这些指令通常是抽象的，例如“把两个光盘放进卧室的保险箱”，并且不包含具体的行动步骤。

   **高层控制器（High-level Controller）生成子目标**：高层控制器（$\pi_H$）根据输入的高层次指令生成一系列子目标。每个子目标代表执行高层次任务所需的一步，例如“拿起光盘”、“打开保险箱”等。这些子目标可以被看作是整个任务分解成更小的、可实现的部分。

   **低层控制器（Low-level Controller）执行子目标**：低层控制器（$\pi_L$）根据当前子目标执行具体的操作。这包括移动到目标位置、操控物体（如打开、拾取）等。低层控制器通过近距离推理，逐步完成每个子目标。

   **观察模型更新空间语义表示**：执行动作之后，机器人从环境中接收新的观测数据（例如 RGB 图像），并通过观察模型更新其空间语义表示。这一表示存储了环境中各对象的位置和状态信息，以及机器人自身的位置等。这使得机器人在需要时可以对过去观察到的对象进行推理，即使这些对象已经不在视野中。

   **高层和低层控制器的动态调整**：基于更新后的空间语义表示，高层控制器可以评估当前任务的进展情况，并决定接下来要执行的子目标（例如，如果某个子目标未成功，则可能重新尝试或选择其他子目标）。同样，低层控制器利用更新的环境信息继续完成指定的子目标，直到其成功或失败。

3. ==综述==，描述了NLP与强化学习的结合，将相关的工作分为两类：自然语言条件下的RL和自然语言辅助RL。本文值得回头重看

4. 机器翻译的工作

5. 训练人工神经网络驾驶汽车

6. 一个端到端的、可训练的、基于门控注意力机制的指令理解系统。模型的输入是游戏环境（pixel）和自然语言指令，输出策略分布$\pi$. 游戏环境和自然语言通过gate-attention模块进行融合，输出状态标识，然后通过A3C的策略学习模块进行策略学习。

7. 一种language-goal-behavior（LGB）的强化学习架构。智能体首先学习G-B过程，从环境中获取goal，然后理解goal并转换成行动；第二阶段，从language中学习goal；最后在执行时，将自然语言instruction中的goal和环境中获取的goal叉乘输入智能体算法来获得行为

8. 2020年 deepmind 类似于6，融合language和vision的特征输入RL模型，输出策略；输入language的使用大语言模型，使得模型能理解zero-shot的语言

9. 2021年 MIT 提出一种使用离散信息作为通信协议的智能体之间的通信方法。人类的自然语言其实是离散形式的，因此多智能体系统使用这种离散形式的协议，可以训练完成后使用聚类算法进行翻译，然后使得人类可以加入协作。具体实现方式：将[observation:对于消息注意力:连续消息:离散消息]作为一个输入，输入PPO等算法进行强化学习训练。最后使用聚类算法，使得人类能够将智能体的离散消息映射到实际意义上，当人类参与协作时可以使用这些离散语言。

10. 双向RNN实现的自编码器，用于神经机器翻译

11. 翻译系统综述，值得参考

12. 2019年 康奈尔 一个视觉+指令知道的智能体强化学习model。模型分为两部分：第一部分，observation融合NL指令生成对于目标位置的特征；第二部分，模型输入目标位置的特征，经过RNN处理，输出对应的动作。前者使用SL，后者RL。本文测试环境值得记录一下，属于人机协作中人指导agent执行动作，agent理解。

13. 阅读过，智能体之间通信的工作，双方玩猜图片的游戏，双方使用图片的特征进行交流

14. 葡萄牙波尔图大学IJCNN A3C2模型，多加了一个通信网络的、多智能体的A3C，智能体之间的通信

15. 神经机器翻译的综述，包含对embedding算法的总结

16. nips2019 UCL 文中提出两种目标函数，用在智能体之间MARL通信。两个目标函数分别作用于speaker和listener。前者直观上提高speaker的说话积极性；后者让listener听到message之后积极采取不一样的动作。

17. 没太看懂，但感觉不涉及人机交互问题

18. 