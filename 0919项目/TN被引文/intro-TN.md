1. 本文开发了一个方法，将智能体游戏环境中的state和action转换成自然语言。环境选取的是青蛙过河。翻译方法：首先，人类玩这个游戏，在玩的过程中采集自然语言的数据集；然后，使用数据集语料库+encoder-decoder根据state和action生成解释。弱点：方法过于简单，且文章中没有公式

2. 2022年 CMU 在对话系统中，智能体通过人类的语言推断出背后隐藏的奖励模型。使得智能体具备一种能力：不仅根据人类的语言完成人类的工作，还能根据之前的上下文信息优化工作的完成。比如说订票系统，不仅能根据语言筛选出合适的机票，还能从符合条件的机票中选择人类最偏好的。模型可以从自然语言中推断出奖励函数，在重复的交互中不断提高奖励估计，并使用推断的奖励在僵持环境中准确选择最佳行动。简单来说，是通过推断人类语言中的偏好，实现IRL的效果。**工作内容不错，但agent揣测人类intent可能不是现阶段的重点**。、

3. 2023 ==CMU==目前为止第一个对agent生成的通信模型进行可解释性转换的算法，效果是对通信model进行可解释化，这是本文的第一个贡献；本文的第二个贡献：验证了通信的稀疏性有助于提高通信效果。本文首先提出了一个类似于IC3Net的agent通信模型，然后使用这个通信模型进行了两个环境的测试。这个通信模型将lstm的隐变量压缩编码成向量，作为通信信息结合门控单元发送给其他智能体。在本文第四章，作者将这些向量化的通信信息映射到二维空间上，发现这些信息确实编码了部分可观察环境中的位置信息，上述作为通信可解释性的一部分。本文第五章描述了人类如何与刚形成的智能体通信。人类通过学习agent训练生成的通信符号，并使用这些符号进而与智能体进行协作。

4. 2020年 google 提出了一种训练框架，能够在不预先定义reward model和模仿学习的情况下，智能体通过和人类进行**非结构化通信的方式**从人类处进行训练（从人类处获得需要完成任务的任务信息），训练好的agent可以作为人类的助手帮助人类完成一些工作。从具体实现上来讲，在训练的过程中，主智能体和从智能体放在同一个网络中训练策略，以此作为联合训练；对于主智能体和从智能体之间的部分可观察的范围限制有所不同，以此限制主智能体和从智能体的区别。总结来看，这篇文章的内容不算很重要，同样也是智能体通过协作和交互来猜其他智能体的奖励函数，交互的方式是放在一起训练（DRQN）

5. 同1

6. ==系统训练了一个问答环境下使用自然语言进行猜图片的智能体架构==。智能体分为两部分：q-bot和a-bot。两个bot都编码历史对话信息，qbot根据历史信息提问；abot根据历史信息、问题和图片进行回答，q来猜a的图片是哪个。双方对话使用自然语言。**训练过程**：首先，双方使用数据集通过SL进行隔离训练，除了语句的MSE之外，q-bot还通过猜出图片的MSE训练；其次，双方使用强化学习self-play训练；最后，由于上述方法训练出的智能体产生的自然语言不够能够使人类理解，因此引入“社区化”训练，即qa过程引入一对多和多对一的RL训练。**本文贡献**：创建了一种自然语言交流的方式。

7. 综述，总结了至今（2023年）对于human-robot teaming现存的工作和挑战，其中包括communication的挑战

8. 提出一个框架，对深度神经网络分类器中的某些神经元的作用进行判别，能够识别出某个神经元能够分类图片的哪个部分。**属于图像分类器的可解释性**。

9. 斯坦福 nips 2020 对分类模型中的神经元生成组合式解释的方法。对于已经完成训练的分类神经模型，对于每个神经元，首先找到使其激活的特征，然后使用启发式搜索方法找到符合其特征的原子标签，然后随着启发式束搜索，逐渐扩展原子标签，将原子标签相互之间使用逻辑连接词连接在一起，构成对神经元对应特征的类似自然语言解释。文章提出的方法是对分类模型的神经元进行事后的解释生成，有助于对推理和决策的可解释性。

10. 本文提出了一种基于互信息的evaluation函数，这个函数通过计算用户的指令信号与状态转移之间的互信息来衡量用户指令信号对状态转移的影响，进而评估这个interface的效果具体如何。对interface进行评估有助于量化用户指令的杂讯，进而提升interface的水平并协作效率

11. 关于模型可解释性的工作。本模型提出了一种模块化的方法，试图解释模型的推理过程。Stack Neural Module Networks (Stack-NMN) 的训练流程设计目的是在不依赖专家指导的情况下，通过模块化的方式进行推理任务的训练。以下是该模型的训练流程的关键步骤：

    ### 1. **输入数据的编码**

    - **问题编码**：首先，输入的问题通过双向LSTM（BiLSTM）编码为一个维度为d的隐藏状态序列 [h1,h2,…,hS][h_1, h_2, \dots, h_S][h1,h2,…,hS]，其中 SSS 是问题中的词的数量。每个词的隐藏状态是前向和后向LSTM的输出的连接结果。
    - **图像编码**：图像被输入到一个预训练的卷积神经网络（CNN）中（如ResNet-101），该CNN负责提取图像的特征图（feature map），作为视觉输入。

    ### 2. **模块布局的预测**

    - **布局控制器（Layout Controller）**：模型通过一个布局控制器，将问题分解为多个子任务。具体地，布局控制器会预测每个时间步（t）下需要执行的模块的权重 w(t)\mathbf{w}(t)w(t)，并为每个模块生成文本参数 ct\mathbf{c}_tct。这些模块包括如“查找”、“过滤”、“比较”等功能，用来解决特定的子任务。
    - **软布局选择**：Stack-NMN采用“软布局”，这意味着在每个时间步，所有模块都被执行，但每个模块的输出会根据其预测的权重 w(t)\mathbf{w}(t)w(t) 进行加权平均。这使得整个推理过程是可微的，能够通过反向传播进行训练，而不依赖强化学习方法。

    ### 3. **模块的执行与推理**

    - **模块执行**：每个模块根据输入的图像特征图和文本参数执行其子任务。模块的输出可以是新的图像注意力区域或其他中间推理结果。每个时间步的模块执行结果会通过一个可微的栈（LIFO栈）进行存储与传递，以便在后续的推理步骤中引用。
    - **栈操作**：模块可以从栈中“弹出”之前的推理结果，并将新的推理结果“压入”栈中。栈的结构确保了推理过程的层次化，使得模块之间可以相互依赖和传递信息。

    ### 4. **损失函数与训练目标**

    - **任务损失**：在VQA任务中，模型的最终目标是预测问题的正确答案。模型在推理过程中生成的答案得分通过softmax交叉熵损失与真实答案进行比较。在REF（指代表达定位）任务中，模型的目标是生成一个正确的边界框，并通过边界框回归损失和图像注意力损失来优化定位精度。
    - **无专家布局监督的训练**：Stack-NMN与传统的模块化网络不同，能够在没有专家指导的情况下训练布局策略。模型的所有参数（包括布局控制器的参数、模块的参数和栈的参数）通过任务的监督信号进行端到端的反向传播优化。

    ### 5. **联合训练（多任务学习）**

    - **VQA和REF的联合训练**：Stack-NMN可以通过共享模块来同时处理多个任务。通过多任务学习，模型能够从不同任务中学习到通用的推理步骤，例如“查找物体”或“处理关系”。这些通用步骤能够在不同任务之间共享，从而提高模型在多个任务上的表现。

    ### 6. **测试与推理**

    - **软布局到离散布局的转换**：在测试阶段，模型的软布局选择可以通过最大化选择（argmax）离散化为单一的模块布局。这种离散化的选择与训练时的软布局一致，模型依然能够保持良好的推理性能。

    ### 总结

    该创新方法的训练流程通过软布局的模块化推理方式实现了端到端的可微训练，不需要依赖专家的布局监督。这种设计不仅提升了推理的透明度和可解释性，还保持了较高的任务精度，能够有效地处理复杂推理任务如视觉问答和指代表达定位。

12. 设计了两个实验，用来验证人类-robot队伍下，人类如何感知机器人之间的通信。实验证明，在人类在场的情况下，机器人之间的交流最好通过自然语言（纯实验）

13. 三页的综述

14. 本文描述了团队沟通协作的一些实验。实验表明，高效的智能体团队在沟通时需要谨慎的交换信息；信息内容和发送时机需要结合对其他智能体内部状态的揣测。高效的智能体团队交流的信息往往是与目标（或自己的意图）相关的信息，而不是与世界状态相关的信息