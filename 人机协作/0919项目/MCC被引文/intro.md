1. 腾讯timi，以人类为中心的RL。智能体不能将游戏的回报作为学习的目标，而是应该更倾向于照顾人类的游戏体验。本文提出了一种以人类为中心的智能体回报的建模体系。首先通过SL用人类数据训练出人类policy。自我为中心的agent和人类的游戏数据作为baseline。agent策略的目的是最大化当前策略和人类合作得到的return对比baseline的优势。共两个网络，一个策略网络，一个估计return的value网络。
1. facebook 能够玩《外交》游戏的agent。这是一个需要玩家发言相互磋商谈判的游戏。首先收集了大量的游戏数据，对已有的预训练模型做微调，使得智能体能够说出游戏中的对话。在数据集中标注好每个对话所对应的意图，便于智能体将意图和自然语言做对应。模型有两个部分组成：策略规划模块和对话模块。对话模块是pretrained语言模型+使用人类玩游戏历史微调的语言模型；输入对话历史和strategy输出的intent，输出新的语言。strategy模块的核心是value网络和policy网络组成的self-play RL智能体。输入标注好意图的对话历史和state，利用rl做决策，输出当前agent的intent和具体action。意图intent：发送者和接受者最有可能采取的一组行动。意图和message发送模块相结合，发出内容可控的且具有泛化性的信息。
1. 清华 2024 训练一个ToM模型，在human-ai合作任务中预测ai agent的下一步动作（意图），增强agent的可解释性。本文未涉及到agent的主动通信。训练过程：SL。首先通过数据集将action和state的embedding结合起来，对其使用transformer输出当前状态下下一个动作的预测。属于外部模型帮助human预测与之合作的智能体的意图。该文章related work有引文

> 我打算将人机合作方向的论文分为两个方面，意图推断和通信。区别在于双方主动进行通信；智能体通过某种手段获取其他人的意图

4. UCLA 2019 science robostic 构建了触觉模型和符号动作模型两种模型，使得机器人模仿人类生成动作。并基于生成的动作序列生成每一步的解释。触觉模型能够使得机器人仿照人类的触觉，如压力和扭力，输出动作；而符号动作模型能够预测动作序列的下一个动作，符号动作模型首先将人类的动作序列视作句子，根据句子生成语法规则，再使用语法规则和已有的动作序列预测下一个动作是什么。动作生成的同时也对当前采取的动作进行解释和动画演示。**这属于什么**，动作生成的时候就带有可解释性，具体解释的时候直接输出动作就好。便于人类理解机器人的意图。
4. 本文提出了一种机器人和操作者双向通信的架构。人类下发指令，指令包括任务目标和约束等。机器人接受任务后首先在模拟环境中演练一下，如果可行则执行，并发送反馈信息给人类；如果不可行，则发送信息给人类希望不执行或调整约束。**双向通信信道是商业化的语音助手，此处无创新**。
4. 
