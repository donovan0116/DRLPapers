1. 腾讯timi，以人类为中心的RL。智能体不能将游戏的回报作为学习的目标，而是应该更倾向于照顾人类的游戏体验。本文提出了一种以人类为中心的智能体回报的建模体系。首先通过SL用人类数据训练出人类policy。自我为中心的agent和人类的游戏数据作为baseline。agent策略的目的是最大化当前策略和人类合作得到的return对比baseline的优势。共两个网络，一个策略网络，一个估计return的value网络。
1. facebook 能够玩《外交》游戏的agent。这是一个需要玩家发言相互磋商谈判的游戏。首先收集了大量的游戏数据，对已有的预训练模型做微调，使得智能体能够说出游戏中的对话。在数据集中标注好每个对话所对应的意图，便于智能体将意图和自然语言做对应。模型有两个部分组成：策略规划模块和对话模块。对话模块是pretrained语言模型+使用人类玩游戏历史微调的语言模型；输入对话历史和strategy输出的intent，输出新的语言。strategy模块的核心是value网络和policy网络组成的self-play RL智能体。输入标注好意图的对话历史和state，利用rl做决策，输出当前agent的intent和具体action。意图intent：发送者和接受者最有可能采取的一组行动。意图和message发送模块相结合，发出内容可控的且具有泛化性的信息。
1. 清华 2024 训练一个ToM模型，在human-ai合作任务中预测ai agent的下一步动作（意图），增强agent的可解释性。本文未涉及到agent的主动通信。训练过程：SL。首先通过数据集将action和state的embedding结合起来，对其使用transformer输出当前状态下下一个动作的预测。属于外部模型帮助human预测与之合作的智能体的意图。该文章related work有引文

> 我打算将人机合作方向的论文分为两个方面，意图推断和通信。区别在于双方主动进行通信；智能体通过某种手段获取其他人的意图

4. 
