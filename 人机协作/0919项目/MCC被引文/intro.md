1. 腾讯timi，以人类为中心的RL。智能体不能将游戏的回报作为学习的目标，而是应该更倾向于照顾人类的游戏体验。本文提出了一种以人类为中心的智能体回报的建模体系。首先通过SL用人类数据训练出人类policy。自我为中心的agent和人类的游戏数据作为baseline。agent策略的目的是最大化当前策略和人类合作得到的return对比baseline的优势。共两个网络，一个策略网络，一个估计return的value网络。
1. facebook 能够玩《外交》游戏的agent。这是一个需要玩家发言相互磋商谈判的游戏。首先收集了大量的游戏数据，对已有的预训练模型做微调，使得智能体能够说出游戏中的对话。在数据集中标注好每个对话所对应的意图，便于智能体将意图和自然语言做对应。模型有两个部分组成：策略规划模块和对话模块。对话模块是pretrained语言模型+使用人类玩游戏历史微调的语言模型；输入对话历史和strategy输出的intent，输出新的语言。strategy模块的核心是value网络和policy网络组成的self-play RL智能体。输入标注好意图的对话历史和state，利用rl做决策，输出当前agent的intent和具体action。意图intent：发送者和接受者最有可能采取的一组行动。意图和message发送模块相结合，发出内容可控的且具有泛化性的信息。
1. 清华 2024 训练一个ToM模型，在human-ai合作任务中预测ai agent的下一步动作（意图），增强agent的可解释性。本文未涉及到agent的主动通信。训练过程：SL。首先通过数据集将action和state的embedding结合起来，对其使用transformer输出当前状态下下一个动作的预测。属于外部模型帮助human预测与之合作的智能体的意图。该文章related work有引文

> 我打算将人机合作方向的论文分为两个方面，意图推断和通信。区别在于双方主动进行通信；智能体通过某种手段获取其他人的意图
>
> 关于意图推断方面，大多数文章聚焦于机器人领域，即机器人在接收到意图之后如何将其转化成运动学方程（5）有些工作是如何将机器人任务分解为动作，并将动作标签展示出来（4）。

4. UCLA 2019 science robostic 构建了触觉模型和符号动作模型两种模型，使得机器人模仿人类生成动作。并基于生成的动作序列生成每一步的解释。触觉模型能够使得机器人仿照人类的触觉，如压力和扭力，输出动作；而符号动作模型能够预测动作序列的下一个动作，符号动作模型首先将人类的动作序列视作句子，根据句子生成语法规则，再使用语法规则和已有的动作序列预测下一个动作是什么。动作生成的同时也对当前采取的动作进行解释和动画演示。**这属于什么**，动作生成的时候就带有可解释性，具体解释的时候直接输出动作就好。便于人类理解机器人的意图。
4. 本文提出了一种机器人和操作者双向通信的架构。人类下发指令，指令包括任务目标和约束等。机器人接受任务后首先在模拟环境中演练一下，如果可行则执行，并发送反馈信息给人类；如果不可行，则发送信息给人类希望不执行或调整约束。**双向通信信道是商业化的语音助手，此处无创新**。
4. 使用增强现实技术使得机器人交流自己的动作意图，纯实验性质 2017年
4. 2022年 科罗拉多大学 构造了一个人机合作的环境，无人机具有全局视野，辅助人类进行挖矿。双方沟通的内容是全局环境的概率质量函数（PMF），即全局状态的价值如何。人机双方都是MDP问题。本文通过设计双方的奖励函数，通过不断交流PMF增强人类对环境的理解。通信方式是：AR。无人机将全局环境的PMF转化成AR发送给人类，人类会发现某些地块更亮，这表示某些地块的价值越高；或者无人机干脆将价值高的地块用导引线连接在一块。
4. science robotics UCLA 2022 人机合作任务中使用交流进行人机价值对齐——XAI+intent推测（未完）
4. 2012年 纯讲实验的，不重要、
4. 2020年 布朗大学 建立了一个测试环境，测试人类的手势、眼神等表达和自然语言相比哪个比较适合让机器人理解人类的意图。实验结果证明后者更合适（未完）
