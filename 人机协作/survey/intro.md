1. 自动化所，解决大规模baseLLM的agent的决策制定，其中agent的数目不固定。为了解决合作问题。使用AC框架，中心化critic，分布式actor
2. natural language-based reinforcement learning，引入task language概念，将nl翻译为agent能够理解的tl，然后执行，并且具备tl生成的模块
3. 腾讯AIlab，训练新的联盟学习玩星际，使得智能体有目的性的挖掘自身联盟的弱点（在星际中，不完全或不完美信息可以被侦察出来，与麻将不同），并激励智能体更倾向于侦察，通过更少的资源训练更好的AI。
4. ICML2021，COPA算法，教练带球员，1)对教练和球员都采用注意机制;2)提出一个变分目标来规范学习;3)设计一种自适应的沟通方式，让教练决定何时与球员沟通。motivation：协调成员动态变化的队伍。
5. 腾讯AI Lab，训练以人为本的智能体和人类组成的多智能体系统。（**或许可以结合偏好强化学习**）motivation：以自己为中心的智能体只能和人类一起走向胜利，但不能以人类为中心。例如都是打王者胜利，但ai抢人类人头会让人类玩家不爽。这里加上了“以人类为中心”的增强。方法：首先人类根据自己的目标创建表现，称为“baseline”；训练value网络来评估人类玩家达到目标得到的return。其次训练gain网络评估agent以人为本为人类带来的积极gain，最后根据agent产生的积极性gain对agent微调。
6. 新加坡NUS，FRL框架。motivation：rl面临训练样本量不足且没法分享的问题，frl可以使得rl智能体相互分享轨迹，增强学习成功的可能性。该文章提出的框架解决了FRL收敛性分析和系统容错率（容错率包括agent随机故障和可能的攻击）问题。
7. Deepmind，RLHF。motivation：在没有程序化奖励模型的基础上训练agent。由人类的偏好指导agent的训练方向。方法：基于模仿学习baseline，通过人类偏好训练建立reward model

> 目前为止，认为HAC&MARL可以分为两类：
>
> 1. 由人类参与指导，如偏好强化学习、RLHF、AI lab的以人类为中心。旨在训练中就引入人类的部分
> 2. 不由人类指导，完全AI内部自行博弈，如魔改架构（COPA），相互博弈等。
> 3. 人类在训练过程不参与指导，但是在执行过程AI需要理解人类的指令。该过程可以与LLM相结合

8. 

open speil google football