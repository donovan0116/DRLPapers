1. 自动化所，解决大规模baseLLM的agent的决策制定，其中agent的数目不固定。为了解决合作问题。使用AC框架，中心化critic，分布式actor

2. natural language-based reinforcement learning，引入task language概念，将nl翻译为agent能够理解的tl，然后执行，并且具备tl生成的模块

3. 腾讯AIlab，训练新的联盟学习玩星际，使得智能体有目的性的挖掘自身联盟的弱点（在星际中，不完全或不完美信息可以被侦察出来，与麻将不同），并激励智能体更倾向于侦察，通过更少的资源训练更好的AI。

4. ICML2021，COPA算法，教练带球员，1)对教练和球员都采用注意机制;2)提出一个变分目标来规范学习;3)设计一种自适应的沟通方式，让教练决定何时与球员沟通。motivation：协调成员动态变化的队伍。

5. 腾讯AI Lab，训练以人为本的智能体和人类组成的多智能体系统。（**或许可以结合偏好强化学习**）motivation：以自己为中心的智能体只能和人类一起走向胜利，但不能以人类为中心。例如都是打王者胜利，但ai抢人类人头会让人类玩家不爽。这里加上了“以人类为中心”的增强。方法：首先人类根据自己的目标创建表现，称为“baseline”；训练value网络来评估人类玩家达到目标得到的return。其次训练gain网络评估agent以人为本为人类带来的积极gain，最后根据agent产生的积极性gain对agent微调。

6. 新加坡NUS，FRL框架。motivation：rl面临训练样本量不足且没法分享的问题，frl可以使得rl智能体相互分享轨迹，增强学习成功的可能性。该文章提出的框架解决了FRL收敛性分析和系统容错率（容错率包括agent随机故障和可能的攻击）问题。

7. Deepmind，RLHF。motivation：在没有程序化奖励模型的基础上训练agent。由人类的偏好指导agent的训练方向。方法：基于模仿学习baseline，通过人类偏好训练建立reward model

8. 人大高瓴，LLM-based HAC，motivation：提出一种基于LLM的与人类进行协作的智能体，并且该智能体能够判断什么时候该让人类介入。人类把握大方向，agent实现子领域。

9. 清华，zero-shot cooperation assume human bias。motivation：已有的zero-shot不需要人类数据就能训练出MARL的agent，但是这种训练只是根据环境reward建模，人类的偏好和环境的reward model不一致。本文借助人类偏见（bias），生成一个有效的策略池。（之前为了防止self-play进入过拟合，多个智能体一起训练形成一个策略池，然后智能体针对策略池自适应调整，预防进入次优）方法：在第一阶段构造策略池时就将隐藏的人类回报放入。

10. US Army，测量MARL系统中多智能体协作的质量。方法：提出一种指标CCM，将智能体的一段行为视为时间序列，然后embedding。embedding之间的影响就是多智能体协作质量的指标。

11. MIT，motivation：研发一个框架，将人类和ai的行为对齐，有助于生成式ai的发展。方法：放弃使用传统的state和action，而是使用更高级的task、行为流形behavioral manifold。通过完成task、比较人机行为来对齐。本质上还是训练期就人机交互。

12. 弗吉尼亚大学，开发对MARL的解释性框架，让人类理解MARL系统的策略。方法：将策略罗列出来；根据人类的自然语言问询来过滤出人类想查询的具体策略，并给出。

13. 微软研究院与滑铁卢大学，motivation：以人类为中心的AI在合作时接受人类的自然语言指示，AI根据这些问题理解、提问澄清和执行。

    ---

14. 清华与腾讯ai，motivation：zero-shot AI与没见过的人类合作。对于新的种群中，智能体个体多样性和结对（pairwise）多样性，提出一个种群熵优化目标，增强多智能体学习水平，使得训练出的智能体和未知的人类合作

15. UCB, AvE, motivation：AI协助人类时，经常需要推测人类的目标，可能导致推测不准，或者哪怕推测准了也不知道该帮助人类到什么程度。在本文中，提出了一种empowerment的度量。AI不需要推测人类的目标，只需要通过协助人类，使得人类在该环境下变得更强就行了。empowerment负责衡量人类是否变得更强。本文提出三个贡献：通过empowerment形式化建模AI对人类的协助、与“目标推测”相比较、提出一种empowerment的加速计算方法。

16. DeepMind，开发Fictitious co-play算法。motivation：AI与人类合作的时候，往往self-play无法学到与novel的人类合作。FCP算法基于self-play，在每次self-play训练的时候，通过随机化神经网络的参数使得agent获得随机性，再给游戏过程打上断点。最后训练一个agent，在这些断点处打赢所有的随机化agent。以此来模仿novel的人类。

17. Deepmind，开发基于种群和meta-learning的协议学习。在此之前，ai可以和之前遇见过的ai合作，但是无法和没遇见过的相互沟通。在之前的工作中，ai之间构造种群，在种群相互接触合作的过程中学习通信协议。但是这种方式只能学到专用的语言协议，无法通过自然语言和人类交流，因此在前人的基础上开发动态种群+meta-learning，增强广泛的适用度。（该文章的引文有需要看的）

18. Openai 2018年，开发一种环境，所有AI通过一种自创的简单符号逻辑语言作为合作的媒介，最后发现他们成功从中找到了规律并且能用其交流，从经验中抽象出组合语言且没有人类数据的帮助，在语言不足的时候还会引入动作补充。输出就是这个MARL的环境和实验结果。

19. MIT，比较了一下人类和rule-based和learning-based的AI合作，验证了人类更倾向于和rule-based的AI合作（由于舒适度等原因）。

20. DeepMind, AlphaStar

21. 腾讯AI Lab，对于MOBA游戏中的AI，微观的策略可以很好的学习，但是宏观的策略不好学。本文开发了分层的宏观策略学习方法，通过监督学习游戏回放的方式（去年12月的论文用过）。整体论文模型分为两部分：parse和attention。前者负责识别当前进行到了游戏的哪个阶段，后者结合游戏当前的阶段，对某种指令进行注意力机制。比如在游戏前期，应该更关注当前兵线。

22. 2018年deepmind，基于种群的方法训练CTF游戏的AI。使得AI不仅能够对固定地图做反应，还能随着地图和队友的变化而改进自己。基于种群的思路，AI跟着种群一起进化，在种群间采用self-play。AI进化出内部奖励模型。

23. 2021年，腾讯AI lab，MGG系统，解决MOBA游戏中策略多样性不足的问题。Macro-Goals guide，该系统首先根据游戏信息建模宏观策略，然后使用高水平玩家的游戏回放对宏观策略进行监督学习，使得AI知道根据自己的阵容、英雄等信息选择不同的策略。具体实现：首先使用当前状态s和macro-goal进行监督学习，得到meta-controller，用来产生macro-goal；使用训练好的meta-controller和得到的macro-goal进行PPO训练，学习到policy。macro-goal怎样得到：macro-goal分为：去哪里和做什么，在某一时刻，通过游戏回放的未来时刻来得到当前时刻去哪里和做什么。![image-20240509164709314](./intro.assets/image-20240509164709314.png)

24. 2020年，南洋理工大学，IMAC系统。本文分为两部分：首先证明了当通信带宽有限时，智能体发送信息的信息熵越低越好，即带宽是智能体信息熵的约束。其次，开发了IMAC算法，该算法使得智能体学会发合适的信息，该信息是一个隐变量模型，输入是智能体的状态，输出为信息向量。对于调度器来说，调度器视作一个虚拟智能体，给所有智能体的信息加权重，最终调度器能够学到低熵有效的信息。![image-20240510221552682](./intro.assets/image-20240510221552682.png)

25. 2016年，纽约大学，CommNet，在多智能体全连接系统中，不仅训练每个智能体的神经网络来输出策略，还训练智能体之间的通信网络的权重参数。所有神经网络都是MLP。![image-20240510231009928](./intro.assets/image-20240510231009928.png)

26. 2004年，马萨诸塞州州立大学，讨论在给定通信成本的条件下，让agent学习通信策略。（未看完）

27. 2016年，牛津大学与DeepMind（未看完）
