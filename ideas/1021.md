## Motivation

本文设立的环境为：人机合作问题，即一个人类和一个智能体共同完成一个目标。

本文需要解决的问题：智能体和人类合作的时候，人类可能会根据态势的改变而变化自己的价值，因此，当人类变化自己的价值时，智能体应当实时调整策略，以适应人类的变化。

## Method

### 第一步

首先设定周期$T$，每经过$T$个时间步，agent收集时间段内人类的所有state-action pair，使用**对抗逆强化学习**的方法得到人类在当前时间步的reward model $V_{t+T}$ 。将$V_{t+T}$和$V_t$相互对比，比较人类在这个周期内是否发生了价值判断的变化。若发生了变化，则说明智能体需要调整自己的策略。

### 第二步

智能体在预先情况下通过self-play得到了若干个策略$(\pi_1,\dots,\pi_n)$。在执行动作时，使用`sample_proportion`依概率对若干个策略选择性地执行。`sample_proportion`是一个维度为$1\times n$的归一化向量，表示选择某一策略的概率。

`sample_proportion`的选择方法：

1. 上一轮的结果中包括`sample_proportion_mu`和`sample_proportion_sigma`。分别表示sample_proportion的均值和方差
2. 按照均值和方差采样得到$M$个sample_proportion（用重参数化保证梯度）。
3. 使用softmax，从M中选择**使得人类的reward model得分最高的sample_proportion**。
4. 将新的策略装载到智能体上。

### 第三步

统计 $T$ 时间内人类的动作频率分布`h`

### 第四步

以`h`为条件，输入`sample_proportion`，使用 conditional-VAE 生成下一阶段的`sample_proportion_mu`和`sample_proportion_sigma`。

这一步的目的是将sample_proportion的生成和人类的动作选择偏好结合在一起。