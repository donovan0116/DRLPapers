## Motivation

本文设立的环境为：人机合作问题，即一个人类和一个智能体共同完成一个目标。

本文需要解决的问题：智能体和人类合作的时候，人类可能会根据态势的改变而变化自己的价值判断（reward model），因此，当人类变化自己的价值判断时，智能体应当实时调整策略，以适应人类的变化。

## Method

### 第一步

首先设定周期$T$，在时间区间 $[t-T, t]$ 内，收集人类的状态-动作对 $\{(s_i, a_i)\}_{i=1}^N$，使用**对抗逆强化学习**的方法得到人类在当前时间步的reward model $V_{t+T}$ 。将$V_{t+T}$和$V_t$相互对比，比较人类在这个周期内是否发生了价值判断的变化。若发生了变化，则说明智能体需要调整自己的策略。

AIRL 是一种基于生成对抗网络（GAN）的逆向强化学习方法，它的目标是从专家示范中恢复奖励函数。

**（1）AIRL 的目标函数：**

AIRL 的判别器 $D_\omega(s, a)$ 被定义为：

$$
D_\omega(s, a) = \frac{\exp\left(f_\omega(s, a)\right)}{\exp\left(f_\omega(s, a)\right) + \pi(a|s)}
$$

其中，$f_\omega(s, a)$ 是可学习的奖励函数参数化，$\pi(a|s)$ 是策略。

**（2）损失函数：**

AIRL 的判别器的目标是区分专家策略和当前策略产生的状态-动作对，其损失函数为：

$$
\mathcal{L}_D = -\mathbb{E}_{(s, a) \sim \text{专家}}[\log D_\omega(s, a)] - \mathbb{E}_{(s, a) \sim \pi}[\log(1 - D_\omega(s, a))]
$$

**（3）策略更新：**

策略 $\pi$ 的更新目标是最大化判别器无法区分其与专家的区别，即最小化以下损失：

$$
\mathcal{L}_\pi = -\mathbb{E}_{(s, a) \sim \pi}[\log D_\omega(s, a)]
$$

**（4）奖励函数的提取：**

在 AIRL 中，学习到的 $f_\omega(s, a)$ 可以视为奖励函数。策略的更新可以使用基于该奖励函数的任何强化学习算法，例如 TRPO、PPO 等。

**（5）整体算法流程：**

- **步骤 1：** 初始化策略 $\pi$ 和判别器参数 $\omega$。
- **步骤 2：** 使用当前策略 $\pi$ 生成状态-动作对，计算判别器损失 $\mathcal{L}_D$，更新 $\omega$。
- **步骤 3：** 使用更新后的判别器，计算新的奖励函数 $f_\omega(s, a)$。
- **步骤 4：** 使用新的奖励函数，更新策略 $\pi$，最小化 $\mathcal{L}_\pi$。
- **步骤 5：** 重复步骤 2-4，直到收敛。

计算当前奖励模型 $v_t$ 与之前的奖励模型 $v_{t-T}$ 之间的差异：

$$
\Delta v = \|v_t - v_{t-T}\|_2
$$

如果 $\Delta v$ 超过预设阈值 $\epsilon$，则认为人类的奖励模型发生了变化，需要调整策略。

---

### 第二步

智能体在预先情况下通过self-play得到了若干个策略$(\pi_1,\dots,\pi_n)$。在执行动作时，使用`sample_proportion`依概率对若干个策略选择性地执行。`sample_proportion`是一个维度为$1\times n$的归一化向量，表示选择某一策略的概率。

`sample_proportion`的选择方法：

**1. 构建策略采样比例的正态分布：**

- 均值向量 $\mu_{t-T} = \text{sample\_proportion\_mu}_{t-T}$
- 协方差矩阵 $\Sigma_{t-T} = \text{diag}(\text{sample\_proportion\_sigma}_{t-T}^2)$

构建多元正态分布：

$$
p_{\text{sample}}(x) = \mathcal{N}(x; \mu_{t-T}, \Sigma_{t-T})
$$

**2. 采样 $M$ 个策略采样比例：**

对于 $i = 1$ 到 $M$：

- **采样：**

  $$
  p_i \sim \mathcal{N}(\mu_{t-T}, \Sigma_{t-T})
  $$

- **归一化：**（确保为有效的概率分布）

  $$
  p_i' = \text{softmax}(p_i)
  $$

**3. 评估采样的策略组合：**

- 计算每个采样比例下的策略在当前人类奖励模型 $v_t$ 下的期望收益：

  $$
  \text{Score}(p_i') = \mathbb{E}_{\pi_{p_i'}} \left[ \sum_{k} \gamma^k v_t(s_k, a_k) \right]
  $$

  其中，$\pi_{p_i'}$ 是按照采样比例 $p_i'$ 混合的策略，$\gamma$ 是折扣因子。

**4. 选择最优策略采样比例：**

- 选择得分最高的采样比例作为当前时刻的策略采样比例：

  $$
  p_t = \arg\max_{p_i'} \text{Score}(p_i')
  $$

---

### 第三步

统计 $T$ 时间内人类的动作频率分布`h`

### 第四步

以`h`为条件，输入`sample_proportion`，使用 conditional-VAE 生成下一阶段的`sample_proportion_mu`和`sample_proportion_sigma`。

**1. 条件 VAE 的输入和输出：**

- **输入：** 当前的策略采样比例 $p_t$ 和人类动作分布 $h$
- **输出：** 下一代的 $\mu_t$ 和 $\Sigma_t$

**2. 条件 VAE 的结构：**

- **编码器：**

  接受 $(p_t, h)$，映射到隐变量空间，得到均值和方差：

  $$
  q_{\phi}(z|p_t, h) = \mathcal{N}\left(z; \mu_z, \Sigma_z\right)
  $$

- **解码器：**

  从隐变量 $z$ 和条件 $h$，生成重构的策略采样比例：

  $$
  p_{\theta}(p_t|z, h)
  $$

**3. 损失函数：**

- **重构损失：**

  $$
  \mathcal{L}_{\text{rec}} = \|p_t - \hat{p_t}\|^2
  $$

  其中，$\hat{p_t}$ 是解码器的输出。

- **KL 散度损失：**

  $$
  \mathcal{L}_{\text{KL}} = \text{KL}\left( q_{\phi}(z|p_t, h) \| p(z|h) \right)
  $$

  一般假设先验 $p(z|h) = \mathcal{N}(0, I)$。

- **总损失：**

  $$
  \mathcal{L} = \mathcal{L}_{\text{rec}} + \beta \mathcal{L}_{\text{KL}}
  $$

  其中，$\beta$ 是权衡参数。

**4. 模型训练：**

- 使用收集的数据对条件 VAE 进行训练，优化参数 $\phi$ 和 $\theta$。

**5. 更新策略采样比例的分布参数：**

- 在推理阶段，给定人类动作分布 $h$，从先验分布采样 $z$，通过解码器生成新的策略采样比例：

  $$
  z \sim \mathcal{N}(0, I)
  $$

  $$
  (\mu_t, \Sigma_t) = \text{Decoder}_{\theta}(z, h)
  $$

- 将新的 $\mu_t$ 和 $\Sigma_t$ 用于下一次循环。

这一步的目的是将sample_proportion的生成和人类的动作选择偏好结合在一起。

---

2024.10.21 11:22am update

首先，根据生成对抗模仿学习（GAIL）的方法，应该是模型输入人类的sa对，输出可能的人类policy函数，随后智能体选择一个和人类的做近似进而选择。但这个和原来的太像了。

其次，选择AIRL，但是在导出reward model的同时还是会涉及到导出policy，不好解释==为什么不全用AIRL==的问题。况且，T周期内的数据量太少了，可能不够生成模型的训练。

第三种更改方式，可以将问题简化，不让智能体配合人类，而是改成环境中有若干个目标，人类在某个时间段选择一个目标作为自己的意图。agent根据sa对推断意图是哪个（能做到，用贝叶斯推断）。推断好之后==agent选择最能够完成该意图的policy==（这一步可以自动构建数据集，预训练一个MLP判断sp的优劣性）。

---

2024.10.23 update

对抗逆强化学习生成reward model存在问题：

+ T周期内数据量不足以训练生成模型
  + 解决方案：更换成推断算法，如$p(\theta|s_{1:i},a_{1:i})$等，具体实现需要数据集，数据集的问题可以通过符号规则生成；场景更换成：“人类和智能体协作，对手是若干个目标，人类选择一个执行，智能体通过人类的动作序列推测人类的目标选择，进而采用适应人类的策略。”
+ 为什么agent需要通过推断人的目标？为什么不能直接给
  + 可能存在无线电静默的场景，或者人类的自然语言对目标变更的描述不够全面，再或者可以**将人类的自然语言描述和推测结合在一起选择智能体的策略**。
+ 做出的推断是事后推断，如何解释有效性？
  + 这个好办，可以统计人类意图变化的频率，选择一个小于这个频率的周期T。
+ 第一步（推测目标）和第二步（根据推测结果选择智能体）的创新已经够了，再往下的创新性有些工作量太大。